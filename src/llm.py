import json
import requests
from openai import OpenAI  # å¯¼å…¥OpenAIåº“ç”¨äºè®¿é—®GPTæ¨¡å‹
from logger import LOG  # å¯¼å…¥æ—¥å¿—æ¨¡å—

class LLM:
    def __init__(self, config):
        """
        åˆå§‹åŒ– LLM ç±»ï¼Œæ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ï¼ˆOpenAI æˆ– Ollamaï¼‰ã€‚

        :param config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰çš„æ¨¡å‹é…ç½®å‚æ•°ã€‚
        """
        self.config = config
        self.model = config.llm_model_type.lower()  # è·å–æ¨¡å‹ç±»å‹å¹¶è½¬æ¢ä¸ºå°å†™
        if self.model == "openai":
            self.client = OpenAI()  # åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
        elif self.model == "ollama":
            self.api_url = config.ollama_api_url  # è®¾ç½®Ollama APIçš„URL
        else:
            LOG.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")  # å¦‚æœæ¨¡å‹ç±»å‹ä¸æ”¯æŒï¼ŒæŠ›å‡ºé”™è¯¯

    def generate_report(self, system_prompt, user_content):
        """
        ç”ŸæˆæŠ¥å‘Šï¼Œæ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„æ¨¡å‹æ¥å¤„ç†è¯·æ±‚ã€‚

        :param system_prompt: ç³»ç»Ÿæç¤ºä¿¡æ¯ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œè§„åˆ™ã€‚
        :param user_content: ç”¨æˆ·æä¾›çš„å†…å®¹ï¼Œé€šå¸¸æ˜¯Markdownæ ¼å¼çš„æ–‡æœ¬ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]

        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ç›¸åº”çš„ç”ŸæˆæŠ¥å‘Šæ–¹æ³•
        if self.model == "openai":
            return self._generate_report_openai(messages)
        elif self.model == "ollama":
            return self._generate_report_ollama(messages)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")

    def _generate_report_openai(self, messages):
        """
        ä½¿ç”¨ OpenAI GPT æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ OpenAI {self.config.openai_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            response = self.client.chat.completions.create(
                model=self.config.openai_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„OpenAIæ¨¡å‹åç§°
                messages=messages
            )
            LOG.debug("GPT å“åº”: {}", response)
            return response.choices[0].message.content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

    def _generate_report_ollama(self, messages):
        """
        ä½¿ç”¨ Ollama LLaMA æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ Ollama {self.config.ollama_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            payload = {
                "model": self.config.ollama_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„Ollamaæ¨¡å‹åç§°
                "messages": messages,
                "max_tokens": 4000,
                "temperature": 0.7,
                "stream": False
            }

            response = requests.post(self.api_url, json=payload)  # å‘é€POSTè¯·æ±‚åˆ°Ollama API
            response_data = response.json()

            # è°ƒè¯•è¾“å‡ºæŸ¥çœ‹å®Œæ•´çš„å“åº”ç»“æ„
            LOG.debug("Ollama å“åº”: {}", response_data)

            # ç›´æ¥ä»å“åº”æ•°æ®ä¸­è·å– content
            message_content = response_data.get("message", {}).get("content", None)
            if message_content:
                return message_content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
            else:
                LOG.error("æ— æ³•ä»å“åº”ä¸­æå–æŠ¥å‘Šå†…å®¹ã€‚")
                raise ValueError("Ollama API è¿”å›çš„å“åº”ç»“æ„æ— æ•ˆ")
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

if __name__ == '__main__':
    from config import Config  # å¯¼å…¥é…ç½®ç®¡ç†ç±»
    config = Config()
    llm = LLM(config)

    markdown_content="""
# Progress for langchain-ai/langchain (2024-08-20 to 2024-08-21)

## Issues Closed in the Last 1 Days
- partners/chroma: release 0.1.3 #25599
- docs: few-shot conceptual guide #25596
- docs: update examples in api ref #25589
"""

    # ç¤ºä¾‹ï¼šç”Ÿæˆ GitHub æŠ¥å‘Š
    # system_prompt = "Your specific system prompt for GitHub report generation"
    system_prompt = """
    Please categorize and distill the projectâ€™s latest updates (including Issues and Pull Requests). Similar or related items should be merged to avoid duplication. The final output should be a structured briefing that includes at least the following three sections:
    New Features: Summarize newly introduced modules or capabilities.
    Key Improvements: Summarize optimizations, enhancements, or standardization efforts to existing functionality.
    Bug Fixes: Summarize resolved defects, errors, or compatibility issues.

    Requirements:
    Keep the wording concise and focus on key information.
    Merge similar or related updates into a single entry.
    Present the output in the style of a briefing note.

    Example Template:

    ğŸ“‘ {Project Name} Progress Briefing
    Date: {Date}

    1. New Features
    Modules & Tools:
    {Example: Added {feature/module/tool name} to support {use case or scenario}}
    Models & Integrations:
    {Example: Added support for {model/service}}
    Other Additions:
    {Example: Added {documentation/example/interface}}

    2. Key Improvements
    Standardization & Compatibility:
    {Example: Enhanced {module/interface} compatibility to support {standard/version}}
    Documentation Enhancements:
    {Example: Updated {documentation/examples} to include {usage instructions/cases}}
    Feature Optimization:
    {Example: Optimized {performance/call logic/database operations} to improve {efficiency/stability}}

    3. Bug Fixes
    Bug Resolutions:
    {Example: Fixed {module/feature} issue causing {error type/exception description}}
    Documentation & Example Corrections:
    {Example: Corrected {documentation/examples} for {typos/syntax errors/parameter issues}}

    âœ… Summary:
    This update primarily focuses on {highlight 1}, {highlight 2}, and {highlight 3}, further improving the frameworkâ€™s {stability/extensibility/usability}.
    """
    github_report = llm.generate_report(system_prompt, markdown_content)
    LOG.debug(github_report)
